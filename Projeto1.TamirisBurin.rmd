---
title: "Projeto 1"
author: "Tamiris Burin"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

### 1. Pacotes necessários e a base de dados

Para este projeto, utilizamos as seguintes *libraries* e bancos de dados

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(knitr)
library(kableExtra)
library(mlr3verse)
library(quanteda)
library(janitor)
library(kknn)
library(randomForest)
library(xgboost)
library(mlr3extralearners)
```

```{r, fig.height=8, fig.width=12, message=FALSE, warning=FALSE}
link <- "https://github.com/FLS-6497/datasets/blob/main/projeto1/discursos_pres_internacionais.csv?raw=true"
discursos <- readr::read_csv2(link) %>% 
  mutate(id = row_number())

linkvalidacao <- "https://github.com/FLS-6497/datasets/blob/main/projeto1/discursos_pres_internacionais_validacao.csv?raw=true"
validacao <- readr::read_csv2(linkvalidacao) %>% 
  mutate(id = row_number())
```

### 2. Modulando Processamentos

Construímos 3 *pipelines* de processamento:

A primeira utilizando as funções:

-   *tolower*: que minimiza a caixa alta de todas palavras para a comparação.

-   *remove_punct*: que remove a pontuação quando normalmente o contexto do corpus não é analisado.

-   *remove_numbers*: que remove os números por nao serem relevantes tambem para a análise.

-   *min_termfreq*: que estipula um valor mínimo de frequência do termo em todos os documentos.

-   *stopwords_language = "pt"*: que remove as palavras pequenas e genericas chamadas de 'stopwords', como 'o', 'a', 'e' etc. por diferenciarem o conteúdo dos textos. Lembrando que usamos o banco de dados de stopwords com a função get_stopwords() para a língua apropriada.

A segunda utilizando todas da primeira e as seguintes:

-   *stem*: que remove diferentes derivacoes dos termos, retornando os termos em seus radicais. Muito util para neutralizar plurais e verbos conjugados.

-   *n=2*: que trata da identificação de *ngrams*. Os *ngrams* representam sequências frequentes de palavras do texto. Podemos especificar que procuramos por cunjuntos de 2 ou mais palavras e estes conjuntos, uma vez identificados, serão analisados como palavras/termos únicos.

E a terceira utilizando todas da anterior, porém substituindo a função de *n_gram* por:

-   *scheme_df* *= 'inverse'*: que podendera o peso das ocorrências de uma palavra pela frequência em que ela aparece em um documento. Isso normaliza os elementos da matriz *bag of words.* Selecionando o tipo *inverse*, é calculada uma proporção da frequência inversa dos termos no documento.

```{r}
processamento1 <- 
  po("textvectorizer", 
     tolower = TRUE,
     remove_punct = TRUE, 
     remove_numbers = TRUE,
     min_termfreq = 30, 
     stopwords_language = "pt") %>>%
  po("scale") %>>% 
  po("mutate") %>>% 
  po("learner", learner = lrn("classif.naive_bayes")) %>% 
  as_learner()
  
processamento2 <- 
  po("textvectorizer", 
     tolower = TRUE,
     remove_punct = TRUE, 
     remove_numbers = TRUE, 
     min_termfreq = 30, 
     stopwords_language = "pt",
     stem = TRUE,
     n=2) %>>%
  po("scale") %>>% 
  po("mutate") %>>% 
  po("learner", learner = lrn("classif.naive_bayes")) %>% 
  as_learner()
  
processamento3 <- 
  po("textvectorizer", 
     tolower = TRUE,
     remove_punct = TRUE, 
     remove_numbers = TRUE,
     min_termfreq = 30,
     stopwords_language = "pt", 
     stem = TRUE,
     scheme_df = 'inverse') %>>%
  po("scale") %>>% 
  po("mutate") %>>% 
  po("learner", learner = lrn("classif.naive_bayes")) %>% 
  as_learner()
```

### 3. Comparando Pré-Processamentos

Agora comparamos os três os pipelines de processamento aplicados à classificação do ***Naive Bayes*** junto da validação ***Accuracy***.

```{r message=FALSE, warning=FALSE, results='hide'}
tsk <- as_task_classif(presidente ~ discurso, data = discursos)

  design.text <- benchmark_grid(
  tasks = tsk,
  learners = list(processamento1, processamento2, processamento3),
  resamplings = rsmp("holdout", ratio = 0.7))
  
resultados.text <- benchmark(design.text)
```

```{r message=FALSE, warning=FALSE, results='hide'}
resultados.text$score(msr(c("classif.acc")))
```

Considerando que os pipelines de processamento n. 1 e n. 3 apresentaram os melhores resultados, rodamos 20 vezes cada com três métricas de validações diferentes: ***Accuracy***, ***Balanced Accuracy***, e o ***Classification Error***.

Foram gerados gráficos para as três métricas citadas.

Lembrando que a interpretação do Classification Error é que quanto menor é seu índice, mais acurado é o resultado.

```{r, message=FALSE, warning=FALSE, results='hide'}
tsk <- as_task_classif(presidente ~ discurso, data = discursos)

validacaooprocessamentos <- function(){
  design <- benchmark_grid(
  tasks = tsk,
  learners = list(processamento1, processamento3),
  resamplings = rsmp("holdout", ratio = 0.7))
  
resultados_pipe <- benchmark(design)
resultados_pipe$score(msrs(c("classif.acc", "classif.bacc", "classif.ce")))
}
```

```{r message=FALSE, warning=FALSE, results='hide'}
resultados_processamentos <- 1:20 %>%
  map_df(~ validacaooprocessamentos())
```

```{r, fig.height=6, fig.width=10,}
#Accuracy
resultados_processamentos %>%
  ggplot(aes(group = nr, y = classif.acc, x = nr)) +
  geom_boxplot()
```

```{r, fig.height=6, fig.width=10,}
#Balanced Accuracy
resultados_processamentos %>%
  ggplot(aes(group = nr, y = classif.bacc, x = nr)) +
  geom_boxplot()
```

```{r, fig.height=6, fig.width=10,}
#Classification Error
resultados_processamentos %>%
  ggplot(aes(group = nr, y = classif.ce, x = nr)) +
  geom_boxplot()
```

Verificamos que a *pipeline* de pré-processameno 3 é a melhor escolha, pois tem maior acurácia e menor erro, em média.

### 4. Pré-Processamento elegido

Enfim, aplicamos na base de discursos o *pipeline* de pré-processamento n. 3 elegido anteriormente:

```{r message=FALSE, warning=FALSE, results='hide'}
processamento <- function(df, var) {
# Cria um corpus
cps <- corpus(df, text_field = var)

# Tokenizacao
tks <- 
  tokens(cps, remove_punct = TRUE, remove_numbers = TRUE) %>%
  tokens_wordstem() %>% 
  tokens_tolower() %>%
  tokens_remove(pattern = stopwords("pt"), min_nchar=4) 

# Criacao de uma matriz bag-of-words
bow <- dfm(tks) %>%
  dfm_trim(min_termfreq = 30)%>% 
  dfm_tfidf(scheme_df = "inverse")

# Transformar o resultado para tibble para o mlr3
dados <- as.matrix(bow) %>%
  as_tibble() %>% 
  janitor::clean_names() %>% 
  mutate_all(as.numeric)

# Definição do target
dados$y <- df$presidente

# Resultado da matrix
return(list(df = dados, bow = bow))

}
processamento(df=discursos, var="discurso")
```

### 5. Adequando as bases de Treino e Teste

Agora fazermos o *split sample* de validação, que neste caso é criar amostras de teste e de treino.

```{r}
#Separando amostras de treino e teste, 70% e 30% respectivamente
discursostreino <- discursos %>% 
  sample_frac(0.7)
discursosteste <- discursos %>% 
  filter(!id %in% discursostreino$id) 

#Processando treino
discursostreinobow <- processamento(df=discursostreino, var= "discurso")

#Adequando teste
discursostestebow <- discursosteste %>% 
  corpus(text_field = "discurso") %>%
  tokens() %>% 
  dfm() %>% 
  dfm_match(featnames(discursostreinobow$bow)) %>%
  as.matrix() %>% 
  as_tibble() %>% 
  janitor::clean_names()%>% 
  mutate_all(as.numeric)
discursostestebow$y <- as.factor(discursosteste$presidente)

#Treino
tsk <- as_task_classif(y ~.,data=discursostreinobow$df)
```

### 6. Modelos

Agora, com as amostras de treino e teste propriamente adequadas, aplicaremos três modelos de classificação para tentarmos predizer o nome do ou da presidente que proferiu cada discurso na amostra de teste:

1.  ***Naive Bayes***

```{r}
naive <- 
  po("learner", learner = lrn("classif.naive_bayes"))  %>%
  as_learner()
```

2.  ***k-Nearest Neighbors (KNN)***

```{r}
kknn <- 
  po("learner", learner = lrn("classif.kknn"))  %>%
  as_learner()
```

3.  ***Random Forest***

```{r}
forest <- 
  po("learner", learner = lrn("classif.randomForest", ntree = 100))  %>%
  as_learner()
```

```{r message=FALSE, results='hide'}
modelos <- benchmark_grid(
  tasks = tsk,
  learners = list(naive, kknn, forest),
  resamplings = rsmp("holdout", ratio = 0.7))

resultadosmodelos <- benchmark(modelos)
```

```{r message=FALSE}
resultadosmodelos$score(msrs(c("classif.acc", "classif.bacc", "classif.ce")))
```

Podemos verificar que o modelo ***Random Forest*** apresentou os melhores resultados.

### 7. Aplicando Ensembles

Para uma abordagem mais sofisticada, podemos combinar diferentes modelos em um mesmo modelo, o que é chamado de *ensemble* em aprendizado de máquina.

Aplicamos duas formas mais comuns de combinação de modelos, ***Bagging*** e ***Boosting***.

1.  ***Bagging*** (com árvores de decisões hipotéticas):

```{r message=FALSE, results='hide'}
#Arvores de decisao
arvore <- 
  po("scale") %>>% 
  po("learner", learner = lrn("classif.rpart", predict_type = "prob")) %>%
  as_learner()

#Bagging (sumsample + bootstrap)
bagging <- 
  po("subsample", frac = 1, replace = T) %>>% 
  po("learner", learner = lrn("classif.rpart")) %>%
  ppl("greplicate", ., 10) %>>% 
  po("classifavg", innum = 10) %>%
  as_learner()

#Treinando as pipelines para a comparacao:
ensemble <- benchmark_grid(
  tasks = tsk,
  learners = list(arvore, bagging),
  resamplings = rsmp("holdout", ratio = 0.7))

resultadosensemble <- benchmark(ensemble)
```

```{r message=FALSE}
resultadosensemble$score(msrs(c("classif.acc", "classif.bacc", "classif.ce")))
```

Verificamos que o modelo de Random Forest ainda parece melhor que o ensemble de *Bagging*.

2.  ***Extreme Gradient Boosting***:

```{r message=FALSE, results='hide'}
#Extreme Gradient Boosting 
gr_xgboost1 <- 
  po("learner", learner = lrn("classif.xgboost", nrounds = 10, predict_type = "prob")) %>%
  as_learner()

gr_xgboost2 <- 
  po("learner", learner = lrn("classif.xgboost", nrounds = 50, predict_type = "prob")) %>%
  as_learner()

gr_xgboost3 <- 
  po("learner", learner = lrn("classif.xgboost", nrounds = 200, predict_type = "prob")) %>%
  as_learner()

modelosxgboost <- benchmark_grid(
  tasks = tsk,
  learners = list(gr_xgboost1, gr_xgboost2, gr_xgboost3),
  resamplings = rsmp("holdout", ratio = 0.7))

resultados <- benchmark(modelosxgboost)
```

```{r message=FALSE}
resultados$score(msrs(c("classif.acc", "classif.bacc", "classif.ce")))
```

Foi possível observar que o método do ***Boosting*** performa de maneira bastante similar ao do ***Random Forest***, com acurácia maior que 0,9. Eventuais diferenças podem ser ruídos decorrentes dos hiperparâmetros deste ensemble.

### 8. Predição

```{r,results='hide'}
#Predicao Random Forest
modeloforest <- forest$train(tsk)
pred1 <- modeloforest$predict_newdata(discursostestebow)
pred1
```

```{r}
#Confere validacao com metricas de teste
pred1$confusion
pred1$score(msr("classif.acc"))
```

```{r,results='hide'}
#Predicao Extreme Gradient Boosting
modelosxgboost <- gr_xgboost2$train(tsk)
pred2 <- modelosxgboost$predict_newdata(discursostestebow)
pred2
```

```{r}
#Confere validacao com metricas de teste
pred2$confusion
pred2 <- pred2$score(msr("classif.acc"))
pred2
```

### 9. Predição de Random Forest e Extreme Gradient Boosting exportando probabilidades:

Para testarmos a predição através das probabilidades expostadas, utilizamos o tipo de predição "***prob***" e a métrica de validação ***logloss*** sob os mesmos modelos acima explorados.

```{r}
forestprop <- 
  po("learner", learner = lrn("classif.randomForest", ntree = 100, predict_type = "prob"))  %>%
  as_learner()

xgboostprop <- 
  po("learner", learner = lrn("classif.xgboost", nrounds = 50, predict_type="prob"))  %>%
  as_learner()
```

```{r,results='hide'}
#Predicao Random Forest (Prop)
modeloforestprop <- forestprop$train(tsk)
pred3 <- modeloforestprop$predict_newdata(discursostestebow)
pred3
```

```{r}
# Confere validação com métricas de teste
pred3$confusion
pred3 <- pred3$score(msr("classif.logloss"))
pred3
```

```{r,results='hide'}
#Predicao Extreme Gradient Boosting (Prop)
modeloxgboostprop <- xgboostprop$train(tsk)
pred4 <- modeloxgboostprop$predict_newdata(discursostestebow)
pred4
```

```{r}
# Confere validação com métricas de teste
pred4$confusion
pred4 <- pred4$score(msr("classif.logloss"))
pred4
```

De acordo com a métrica da acurácia, o modelo ***Boosting*** tende a revelar proporções mais altas de probabilidade de acerto. A métrica do ***logloss*** é o inverso do logaritmo da função de probabilidade, de forma que valores menores de logloss significam maior probabilidade de acerto do modelo. Sendo assim, o método ***Boosting*** é superior ao ***Random Forest***, nesse caso em particular.

### 10. Predição para base de validação com discursos sem indicação de autoria

Testamos, por fim, a acurácia do Extreme Gradient Boost na amostra de validação:

```{r}
#Adequo a base de validacao externa
discursosvalidacaobow <- validacao %>% 
  corpus(text_field = "discurso") %>% 
  tokens() %>% 
  dfm() %>% 
  dfm_match(featnames(discursostreinobow$bow)) %>%
  as.matrix() %>% 
  as_tibble() %>% 
  janitor::clean_names()

#Predicao com o modelo Xgboost por logloss proporcional
predfinal <- modeloxgboostprop$predict_newdata(discursosvalidacaobow)

predtamirisburin <- head(cbind(validacao$id, as.character(predfinal$response)), 25)

#Tabela de Predição da base de validação com discursos sem indicação de autoria
tabelapredtamirisburin <- predtamirisburin %>% 
  kable(
    caption ="<b>Predição - Projeto 1 - Tamiris Burin</b>", 
    col.names = c("Id do discurso","Presidente previsto")) %>%
  kable_styling("striped", full_width = F)
tabelapredtamirisburin
```
